{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 scrapelib is a library for making requests for less-than-reliable websites. Source: https://github.com/jamesturk/scrapelib Documentation: https://jamesturk.github.io/scrapelib/ Issues: https://github.com/jamesturk/scrapelib/issues Features \u00b6 scrapelib originated as part of the Open States project to scrape the websites of all 50 state legislatures and as a result was therefore designed with features desirable when dealing with sites that have intermittent errors or require rate-limiting. Advantages of using scrapelib over using requests as-is: HTTP(S) and FTP requests via an identical API support for simple caching with pluggable cache backends highly-configurable request throtting configurable retries for non-permanent site failures All of the power of the suberb requests library. Installation \u00b6 scrapelib is on PyPI , and can be installed via any standard package management tool: poetry add scrapelib or: pip install scrapelib Example Usage \u00b6 import scrapelib s = scrapelib.Scraper(requests_per_minute=10) # Grab Google front page s.get('http://google.com') # Will be throttled to 10 HTTP requests per minute while True: s.get('http://example.com')","title":"Overview"},{"location":"#overview","text":"scrapelib is a library for making requests for less-than-reliable websites. Source: https://github.com/jamesturk/scrapelib Documentation: https://jamesturk.github.io/scrapelib/ Issues: https://github.com/jamesturk/scrapelib/issues","title":"Overview"},{"location":"#features","text":"scrapelib originated as part of the Open States project to scrape the websites of all 50 state legislatures and as a result was therefore designed with features desirable when dealing with sites that have intermittent errors or require rate-limiting. Advantages of using scrapelib over using requests as-is: HTTP(S) and FTP requests via an identical API support for simple caching with pluggable cache backends highly-configurable request throtting configurable retries for non-permanent site failures All of the power of the suberb requests library.","title":"Features"},{"location":"#installation","text":"scrapelib is on PyPI , and can be installed via any standard package management tool: poetry add scrapelib or: pip install scrapelib","title":"Installation"},{"location":"#example-usage","text":"import scrapelib s = scrapelib.Scraper(requests_per_minute=10) # Grab Google front page s.get('http://google.com') # Will be throttled to 10 HTTP requests per minute while True: s.get('http://example.com')","title":"Example Usage"},{"location":"changelog/","text":"Changelog \u00b6 2.0.6 \u00b6 23 June 2021 set default values for verify & allow_redirects to True (matching requests & expected behavior) 2.0.5 \u00b6 15 June 2021 allow importing SQLiteCache and MemoryCache from scrapelib 2.0.4 \u00b6 13 April 2021 bugfix: params was not carried through properly 2.0.2 \u00b6 9 April 2021 added py.typed 2.0.1 \u00b6 9 April 2021 Python 3.7 installation fix fixed docker image 2.0.0 \u00b6 9 April 2021 removed response.code backwards-compatibility shim dropped Python 2 shims removed/refactored some internal interfaces added first pass at (experimental for now) mypy typing 1.2.0 \u00b6 13 November 2018 add verify parameter to optionally disable SSL verification 1.1.1 \u00b6 16 April 2018 small tweak to warning on retry public email change 1.1.0 \u00b6 6 June 2017 added more resilient retry logic that retries when any type of exception occurs during a request 1.0.2 \u00b6 16 April 2017 add retry on ChunkedEncodingError depend on requests[security] for better SSL handling 1.0.1 \u00b6 16 April 2017 broken release 1.0.0 \u00b6 20 March 2015 drop deprecated urlopen interface documentation thanks to poliquin sqlite cache thanks to poliquin fix for SSLError retries pointed out by Eric Mill 0.10.1 \u00b6 22 January 2015 SQLite cache backend (thanks Chris Poliquin!) test and fix for header merging bug 0.10.0 \u00b6 15 July 2014 added kwarg to use last modified headers when using caching, thanks to Kaitlin Devine fix for chardet bug when opening large PDFs (& other binary files) with urlretrieve - thanks to Katilin Devine deprecation of urlopen in favor of Requests\\'s request(), get(), post(), etc. removal of robots.txt code switch tests to py.test addition of wheel for release 0.9.1 \u00b6 28 March 2014 support kwargs in request paths -thanks to Drew Vogel allow_cookies removal and documentation fix -thanks to Joe Germuska add dir param to urlretrieve -thanks to Alison Rowland 0.9.0 \u00b6 22 May 2013 replace FTPSession with FTPAdapter fixes for latest requests 0.8.0 \u00b6 18 March 2013 requests 1.0 compatibility removal of requests pass-throughs deprecation of setting parameters via constructor 0.7.4 \u00b6 20 December 2012 bugfix for status_code coming from a cache bugfix for setting user-agent from headers fix requests version at <1.0 0.7.3 \u00b6 21 June 2012 fix for combination of FTP and caching drop unnecessary ScrapelibSession bytes fix for scrapeshell use UTF8 if encoding guess fails 0.7.2 \u00b6 9 May 2012 bugfix for user-agent check bugfix for cached content with r characters bugfix for requests >= 0.12 cache_dir deprecation is total 0.7.1 \u00b6 27 April 2012 breaking change: no longer accept URLs without a scheme deprecation of error_dir & context-manager mode addition of overridable accept_response hook bugfix: retry on more requests errors bugfix: unicode cached content no longer incorrectly encoded implement various requests enhancements separately for ease of reuse convert more Scraper parameters to properties 0.7.0 \u00b6 23 April 2012 rewritten internals to use requests, dropping httplib2 as a result of rewrite, caching behavior no longer attempts to be compliant with the HTTP specification but is much more configurable added cache_write_only option deprecation of accept_cookies, use_cache_first, cache_dir parameter improved tests improved Python 3 support 0.6.2 \u00b6 20 April 2012 bugfix for POST-redirects drastically improved test coverage add encoding to ResultStr 0.6.1 \u00b6 19 April 2012 add .bytes attribute to ResultStr bugfix related to bytes in urlretrieve 0.6.0 \u00b6 19 April 2012 remove urllib2 fallback for HTTP rework entire test suite to not rely on Flask Unicode & Str unification experimental Python 3.2 support 0.5.8 \u00b6 15 February 2012 fix to test suite from Alex Chiang 0.5.7 \u00b6 2 February 2012 -p, --postdata parameter argv fix for IPython \\<= 0.10 from Joe Germuska treat FTP 550 errors as HTTP 404s use_cache_first improvements 0.5.6 \u00b6 9 November 2011 scrapeshell fix for IPython >= 0.11 scrapelib.urlopen can take method/body params too 0.5.5 \u00b6 27 September 2011 use None for no timeout, never create non-blocking socket documentation and owernship changes 0.5.4 \u00b6 7 June 2011 actually fix reinstantiation of Http object 0.5.3 \u00b6 7 June 2011 bugfix for reinstantiation of Http object 0.5.2 \u00b6 16 May 2011 support timeout for urllib2 requests 0.5.1 \u00b6 6 April 2011 bugfix for exception handling on retry fix a deprecation warning for Python 2.6+ 0.5.0 \u00b6 18 March 2011 sphinx documentation addition of scrapeshell addition of retry_on_404 parameter to urlopen bugfix to exception handling scope issue bugfix within tests to avoid false negative 0.4.3 \u00b6 11 February 2011 fix retry on certain httplib2 errors add a top-level urlopen function 0.4.2 \u00b6 8 February 2011 fix retry on socket errors close temporary file handle 0.4.1 \u00b6 7 December 2010 support retry of requests that produce socket timeouts increased test coverage 0.4.0 \u00b6 8 November 2010 bugfix: tests require unittest2 or python 2.7 configurable retry handling for random failures 0.3.0 \u00b6 5 October 2010 bugfixes for cookie handling better test suite follow redirects even after a POST change several configuration variables into properties request timeout argument 0.2.0 \u00b6 9 July 2010 use_cache_first option to avoid extra HTTP HEAD requests raise_errors option to treat HTTP errors as exceptions addition of urlretrieve","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#206","text":"23 June 2021 set default values for verify & allow_redirects to True (matching requests & expected behavior)","title":"2.0.6"},{"location":"changelog/#205","text":"15 June 2021 allow importing SQLiteCache and MemoryCache from scrapelib","title":"2.0.5"},{"location":"changelog/#204","text":"13 April 2021 bugfix: params was not carried through properly","title":"2.0.4"},{"location":"changelog/#202","text":"9 April 2021 added py.typed","title":"2.0.2"},{"location":"changelog/#201","text":"9 April 2021 Python 3.7 installation fix fixed docker image","title":"2.0.1"},{"location":"changelog/#200","text":"9 April 2021 removed response.code backwards-compatibility shim dropped Python 2 shims removed/refactored some internal interfaces added first pass at (experimental for now) mypy typing","title":"2.0.0"},{"location":"changelog/#120","text":"13 November 2018 add verify parameter to optionally disable SSL verification","title":"1.2.0"},{"location":"changelog/#111","text":"16 April 2018 small tweak to warning on retry public email change","title":"1.1.1"},{"location":"changelog/#110","text":"6 June 2017 added more resilient retry logic that retries when any type of exception occurs during a request","title":"1.1.0"},{"location":"changelog/#102","text":"16 April 2017 add retry on ChunkedEncodingError depend on requests[security] for better SSL handling","title":"1.0.2"},{"location":"changelog/#101","text":"16 April 2017 broken release","title":"1.0.1"},{"location":"changelog/#100","text":"20 March 2015 drop deprecated urlopen interface documentation thanks to poliquin sqlite cache thanks to poliquin fix for SSLError retries pointed out by Eric Mill","title":"1.0.0"},{"location":"changelog/#0101","text":"22 January 2015 SQLite cache backend (thanks Chris Poliquin!) test and fix for header merging bug","title":"0.10.1"},{"location":"changelog/#0100","text":"15 July 2014 added kwarg to use last modified headers when using caching, thanks to Kaitlin Devine fix for chardet bug when opening large PDFs (& other binary files) with urlretrieve - thanks to Katilin Devine deprecation of urlopen in favor of Requests\\'s request(), get(), post(), etc. removal of robots.txt code switch tests to py.test addition of wheel for release","title":"0.10.0"},{"location":"changelog/#091","text":"28 March 2014 support kwargs in request paths -thanks to Drew Vogel allow_cookies removal and documentation fix -thanks to Joe Germuska add dir param to urlretrieve -thanks to Alison Rowland","title":"0.9.1"},{"location":"changelog/#090","text":"22 May 2013 replace FTPSession with FTPAdapter fixes for latest requests","title":"0.9.0"},{"location":"changelog/#080","text":"18 March 2013 requests 1.0 compatibility removal of requests pass-throughs deprecation of setting parameters via constructor","title":"0.8.0"},{"location":"changelog/#074","text":"20 December 2012 bugfix for status_code coming from a cache bugfix for setting user-agent from headers fix requests version at <1.0","title":"0.7.4"},{"location":"changelog/#073","text":"21 June 2012 fix for combination of FTP and caching drop unnecessary ScrapelibSession bytes fix for scrapeshell use UTF8 if encoding guess fails","title":"0.7.3"},{"location":"changelog/#072","text":"9 May 2012 bugfix for user-agent check bugfix for cached content with r characters bugfix for requests >= 0.12 cache_dir deprecation is total","title":"0.7.2"},{"location":"changelog/#071","text":"27 April 2012 breaking change: no longer accept URLs without a scheme deprecation of error_dir & context-manager mode addition of overridable accept_response hook bugfix: retry on more requests errors bugfix: unicode cached content no longer incorrectly encoded implement various requests enhancements separately for ease of reuse convert more Scraper parameters to properties","title":"0.7.1"},{"location":"changelog/#070","text":"23 April 2012 rewritten internals to use requests, dropping httplib2 as a result of rewrite, caching behavior no longer attempts to be compliant with the HTTP specification but is much more configurable added cache_write_only option deprecation of accept_cookies, use_cache_first, cache_dir parameter improved tests improved Python 3 support","title":"0.7.0"},{"location":"changelog/#062","text":"20 April 2012 bugfix for POST-redirects drastically improved test coverage add encoding to ResultStr","title":"0.6.2"},{"location":"changelog/#061","text":"19 April 2012 add .bytes attribute to ResultStr bugfix related to bytes in urlretrieve","title":"0.6.1"},{"location":"changelog/#060","text":"19 April 2012 remove urllib2 fallback for HTTP rework entire test suite to not rely on Flask Unicode & Str unification experimental Python 3.2 support","title":"0.6.0"},{"location":"changelog/#058","text":"15 February 2012 fix to test suite from Alex Chiang","title":"0.5.8"},{"location":"changelog/#057","text":"2 February 2012 -p, --postdata parameter argv fix for IPython \\<= 0.10 from Joe Germuska treat FTP 550 errors as HTTP 404s use_cache_first improvements","title":"0.5.7"},{"location":"changelog/#056","text":"9 November 2011 scrapeshell fix for IPython >= 0.11 scrapelib.urlopen can take method/body params too","title":"0.5.6"},{"location":"changelog/#055","text":"27 September 2011 use None for no timeout, never create non-blocking socket documentation and owernship changes","title":"0.5.5"},{"location":"changelog/#054","text":"7 June 2011 actually fix reinstantiation of Http object","title":"0.5.4"},{"location":"changelog/#053","text":"7 June 2011 bugfix for reinstantiation of Http object","title":"0.5.3"},{"location":"changelog/#052","text":"16 May 2011 support timeout for urllib2 requests","title":"0.5.2"},{"location":"changelog/#051","text":"6 April 2011 bugfix for exception handling on retry fix a deprecation warning for Python 2.6+","title":"0.5.1"},{"location":"changelog/#050","text":"18 March 2011 sphinx documentation addition of scrapeshell addition of retry_on_404 parameter to urlopen bugfix to exception handling scope issue bugfix within tests to avoid false negative","title":"0.5.0"},{"location":"changelog/#043","text":"11 February 2011 fix retry on certain httplib2 errors add a top-level urlopen function","title":"0.4.3"},{"location":"changelog/#042","text":"8 February 2011 fix retry on socket errors close temporary file handle","title":"0.4.2"},{"location":"changelog/#041","text":"7 December 2010 support retry of requests that produce socket timeouts increased test coverage","title":"0.4.1"},{"location":"changelog/#040","text":"8 November 2010 bugfix: tests require unittest2 or python 2.7 configurable retry handling for random failures","title":"0.4.0"},{"location":"changelog/#030","text":"5 October 2010 bugfixes for cookie handling better test suite follow redirects even after a POST change several configuration variables into properties request timeout argument","title":"0.3.0"},{"location":"changelog/#020","text":"9 July 2010 use_cache_first option to avoid extra HTTP HEAD requests raise_errors option to treat HTTP errors as exceptions addition of urlretrieve","title":"0.2.0"},{"location":"reference/","text":"API Reference \u00b6 Scraper \u00b6 Scraper is the most important class provided by scrapelib (and generally the only one to be instantiated directly). It provides a large number of options allowing for customization. It wraps requests.Session and has the same attributes & methods available. Parameters: Name Type Description Default raise_errors set to True to raise a HTTPError on 4xx or 5xx response required requests_per_minute maximum requests per minute (0 for unlimited, defaults to 60) required retry_attempts number of times to retry if timeout occurs or page returns a (non-404) error required retry_wait_seconds number of seconds to retry after first failure, subsequent retries will double this wait required verify set to False to disable HTTPS verification. required request ( self , method , url , params = None , data = None , headers = None , cookies = None , files = None , auth = None , timeout = None , allow_redirects = True , proxies = None , hooks = None , stream = None , verify = True , cert = None , json = None , retry_on_404 = False ) \u00b6 Override, wraps Session.request in caching. Cache is only used if key_for_request returns a valid key and should_cache_response was true as well. urlretrieve ( self , url , filename = None , method = 'GET' , body = None , dir = None , ** kwargs ) \u00b6 Save result of a request to a file, similarly to :func: urllib.urlretrieve . If an error is encountered may raise any of the scrapelib exceptions _. A filename may be provided or :meth: urlretrieve will safely create a temporary file. If a directory is provided, a file will be given a random name within the specified directory. Either way, it is the responsibility of the caller to ensure that the temporary file is deleted when it is no longer needed. Parameters: Name Type Description Default url str URL for request required filename str optional name for file None method str any valid HTTP method, but generally GET or POST 'GET' body dict optional body for request, to turn parameters into an appropriate string use :func: urllib.urlencode() None dir str optional directory to place file in None Returns: Type Description Tuple[str, requests.models.Response] tuple with filename for saved response (will be same as given filename if one was given, otherwise will be a temp file in the OS temp directory) and a Response object that can be used to inspect the response headers. Caching \u00b6 Assign a MemoryCache , FileCache , or SQLiteCache to the cache_storage property of a scrapelib.Scraper to cache responses: from scrapelib import Scraper from scrapelib.cache import FileCache cache = FileCache('cache-directory') scraper = Scraper() scraper.cache_storage = cache scraper.cache_write_only = False MemoryCache \u00b6 In memory cache for request responses. FileCache \u00b6 File-based cache for request responses. Parameters: Name Type Description Default cache_dir directory for storing responses required check_last_modified set to True to compare last-modified timestamp in cached response with value from HEAD request required SQLiteCache \u00b6 SQLite cache for request responses. Parameters: Name Type Description Default cache_path path for SQLite database file required check_last_modified set to True to compare last-modified timestamp in cached response with value from HEAD request required Exceptions \u00b6 HTTPError \u00b6 Raised when urlopen encounters a 4xx or 5xx error code and the raise_errors option is true. HTTPMethodUnavailableError \u00b6 Raised when the supplied HTTP method is invalid or not supported by the HTTP backend. FTPError \u00b6","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#scraper","text":"Scraper is the most important class provided by scrapelib (and generally the only one to be instantiated directly). It provides a large number of options allowing for customization. It wraps requests.Session and has the same attributes & methods available. Parameters: Name Type Description Default raise_errors set to True to raise a HTTPError on 4xx or 5xx response required requests_per_minute maximum requests per minute (0 for unlimited, defaults to 60) required retry_attempts number of times to retry if timeout occurs or page returns a (non-404) error required retry_wait_seconds number of seconds to retry after first failure, subsequent retries will double this wait required verify set to False to disable HTTPS verification. required","title":"Scraper"},{"location":"reference/#scrapelib.Scraper.request","text":"Override, wraps Session.request in caching. Cache is only used if key_for_request returns a valid key and should_cache_response was true as well.","title":"request()"},{"location":"reference/#scrapelib.Scraper.urlretrieve","text":"Save result of a request to a file, similarly to :func: urllib.urlretrieve . If an error is encountered may raise any of the scrapelib exceptions _. A filename may be provided or :meth: urlretrieve will safely create a temporary file. If a directory is provided, a file will be given a random name within the specified directory. Either way, it is the responsibility of the caller to ensure that the temporary file is deleted when it is no longer needed. Parameters: Name Type Description Default url str URL for request required filename str optional name for file None method str any valid HTTP method, but generally GET or POST 'GET' body dict optional body for request, to turn parameters into an appropriate string use :func: urllib.urlencode() None dir str optional directory to place file in None Returns: Type Description Tuple[str, requests.models.Response] tuple with filename for saved response (will be same as given filename if one was given, otherwise will be a temp file in the OS temp directory) and a Response object that can be used to inspect the response headers.","title":"urlretrieve()"},{"location":"reference/#caching","text":"Assign a MemoryCache , FileCache , or SQLiteCache to the cache_storage property of a scrapelib.Scraper to cache responses: from scrapelib import Scraper from scrapelib.cache import FileCache cache = FileCache('cache-directory') scraper = Scraper() scraper.cache_storage = cache scraper.cache_write_only = False","title":"Caching"},{"location":"reference/#memorycache","text":"In memory cache for request responses.","title":"MemoryCache"},{"location":"reference/#filecache","text":"File-based cache for request responses. Parameters: Name Type Description Default cache_dir directory for storing responses required check_last_modified set to True to compare last-modified timestamp in cached response with value from HEAD request required","title":"FileCache"},{"location":"reference/#sqlitecache","text":"SQLite cache for request responses. Parameters: Name Type Description Default cache_path path for SQLite database file required check_last_modified set to True to compare last-modified timestamp in cached response with value from HEAD request required","title":"SQLiteCache"},{"location":"reference/#exceptions","text":"","title":"Exceptions"},{"location":"reference/#httperror","text":"Raised when urlopen encounters a 4xx or 5xx error code and the raise_errors option is true.","title":"HTTPError"},{"location":"reference/#httpmethodunavailableerror","text":"Raised when the supplied HTTP method is invalid or not supported by the HTTP backend.","title":"HTTPMethodUnavailableError"},{"location":"reference/#ftperror","text":"","title":"FTPError"},{"location":"scrapeshell/","text":"scrapeshell \u00b6 Many times, especially during development, it is useful to open an interactive shell to tinker with a page. Often the HTML being returned is slightly out of sync with what is being seen in the browser, and it can be difficult to detect these differences without firing up an interactive python shell and inspecting what the request is returning. If scrapelib is installed on your path it provides the scrapeshell command. scrapeshell <URL> will open an IPython shell and present you with an instance of requests.Response with the contents of the scraped page and if lxml is installed, an lxml.html.HtmlElement instance as well. scrapeshell \u00b6 scrapeshell arguments \u00b6 url \u00b6 scrapeshell requires a URL, which will then be retrieved via a scrapelib.Scraper.get call. --ua user_agent \u00b6 Set a custom user agent (useful for seeing if a site is returning different results based on UA). --noredirect \u00b6 Don't follow redirects.","title":"scrapeshell"},{"location":"scrapeshell/#scrapeshell","text":"Many times, especially during development, it is useful to open an interactive shell to tinker with a page. Often the HTML being returned is slightly out of sync with what is being seen in the browser, and it can be difficult to detect these differences without firing up an interactive python shell and inspecting what the request is returning. If scrapelib is installed on your path it provides the scrapeshell command. scrapeshell <URL> will open an IPython shell and present you with an instance of requests.Response with the contents of the scraped page and if lxml is installed, an lxml.html.HtmlElement instance as well.","title":"scrapeshell"},{"location":"scrapeshell/#scrapeshell_1","text":"","title":"scrapeshell"},{"location":"scrapeshell/#scrapeshell-arguments","text":"","title":"scrapeshell arguments"},{"location":"scrapeshell/#url","text":"scrapeshell requires a URL, which will then be retrieved via a scrapelib.Scraper.get call.","title":"url"},{"location":"scrapeshell/#-ua-user_agent","text":"Set a custom user agent (useful for seeing if a site is returning different results based on UA).","title":"--ua user_agent"},{"location":"scrapeshell/#-noredirect","text":"Don't follow redirects.","title":"--noredirect"}]}